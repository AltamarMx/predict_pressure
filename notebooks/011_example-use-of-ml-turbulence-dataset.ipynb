{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.007549,
     "end_time": "2021-03-21T17:47:19.382981",
     "exception": false,
     "start_time": "2021-03-21T17:47:19.375432",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Description\n",
    "This notebook demonstrates how to use the dataset. A large set of features and labels is formed. A tensor basis neural network (TBNN) is created using Keras. The network is trained and evaluated on some sample points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006363,
     "end_time": "2021-03-21T17:47:19.396028",
     "exception": false,
     "start_time": "2021-03-21T17:47:19.389665",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loading the dataset features and labels, creating a training/validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T17:47:19.431734Z",
     "iopub.status.busy": "2021-03-21T17:47:19.430881Z",
     "iopub.status.idle": "2021-03-21T17:47:31.524190Z",
     "shell.execute_reply": "2021-03-21T17:47:31.523457Z"
    },
    "papermill": {
     "duration": 12.121625,
     "end_time": "2021-03-21T17:47:31.524397",
     "exception": false,
     "start_time": "2021-03-21T17:47:19.402772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T17:47:19.431734Z",
     "iopub.status.busy": "2021-03-21T17:47:19.430881Z",
     "iopub.status.idle": "2021-03-21T17:47:31.524190Z",
     "shell.execute_reply": "2021-03-21T17:47:31.523457Z"
    },
    "papermill": {
     "duration": 12.121625,
     "end_time": "2021-03-21T17:47:31.524397",
     "exception": false,
     "start_time": "2021-03-21T17:47:19.402772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features and labels from the dataset: kepsilon\n",
      "Shape of basis tensor array: (791490, 10, 3, 3)\n",
      "Shape of invariant features array: (791490, 47)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Object arrays cannot be loaded when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 61\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShape of invariant features array: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(Invariants\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#Load the additional scalars (N,5): \u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#    q[:,0] = Ratio of excess rotation to strain rate,\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m#    q[:,1] = Wall-distance based Reynolds number, \u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#    q[:,2] = Ratio of turbulent time scale to mean strain time scale\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m#    q[:,3] = Ratio of total Reynolds stress to 1/2 * normal Reynolds stress (TKE)\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# f = \"../data/kepsilon/\"\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m Scalars \u001b[38;5;241m=\u001b[39m \u001b[43mloadCombinedArray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcases\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShape of scalar features array: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(Scalars\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m#Combine the invariants and scalars to form a feature set\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [4], line 35\u001b[0m, in \u001b[0;36mloadCombinedArray\u001b[0;34m(cases, field)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloadCombinedArray\u001b[39m(cases,field):\n\u001b[0;32m---> 35\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mdataset\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mdataset\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mcase\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mfield \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m case \u001b[38;5;129;01min\u001b[39;00m cases])\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "Cell \u001b[0;32mIn [4], line 35\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloadCombinedArray\u001b[39m(cases,field):\n\u001b[0;32m---> 35\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mcase\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mfield\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m case \u001b[38;5;129;01min\u001b[39;00m cases])\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/lib/npyio.py:432\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode,\n\u001b[1;32m    430\u001b[0m                                   max_header_size\u001b[38;5;241m=\u001b[39mmax_header_size)\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/lib/format.py:787\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mhasobject:\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;66;03m# The array contained Python objects. We need to unpickle the data.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n\u001b[0;32m--> 787\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObject arrays cannot be loaded when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    788\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_pickle=False\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pickle_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    790\u001b[0m         pickle_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mValueError\u001b[0m: Object arrays cannot be loaded when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "# Loading data - select which cases to include in the training/validation set (commented out cases are held out)\n",
    "cases = ['DUCT_1100',\n",
    "         'DUCT_1150',\n",
    "         'DUCT_1250',\n",
    "         'DUCT_1300',\n",
    "         'DUCT_1350',\n",
    "         'DUCT_1400',\n",
    "         'DUCT_1500',\n",
    "         'DUCT_1600',\n",
    "         'DUCT_1800',\n",
    "         #'DUCT_2000',\n",
    "         'DUCT_2205',\n",
    "         'DUCT_2400',\n",
    "         'DUCT_2600',\n",
    "         'DUCT_2900',\n",
    "         'DUCT_3200',\n",
    "         #'DUCT_3500',\n",
    "         'PHLL_case_0p5',\n",
    "         'PHLL_case_0p8',\n",
    "         'PHLL_case_1p0',\n",
    "         #'PHLL_case_1p2',\n",
    "         'PHLL_case_1p5',\n",
    "         'BUMP_h20',\n",
    "         'BUMP_h26',\n",
    "         'BUMP_h31',\n",
    "         #'BUMP_h38',\n",
    "         'BUMP_h42',\n",
    "         'CNDV_12600',\n",
    "         'CNDV_20580',\n",
    "         'CBFS_13700'\n",
    "         ]\n",
    "\n",
    "#Convenient functions for loading dataset\n",
    "def loadCombinedArray(cases,field):\n",
    "    data = np.concatenate([np.load('../data/'+dataset+'/'+dataset+'_'+case+'_'+field + '.npy') for case in cases])\n",
    "    return data\n",
    "\n",
    "def loadLabels(cases,field):\n",
    "    data = np.concatenate([np.load('../data/'+'labels/'+case+'_'+field + '.npy') for case in cases])\n",
    "    return data\n",
    "\n",
    "# Select RANS model\n",
    "dataset = 'kepsilon' \n",
    "\n",
    "print('Loading features and labels from the dataset: '+ dataset)\n",
    "\n",
    "#Load the set of ten basis tensors (N,10,3,3), from Pope \"A more general effective-viscosity hypothesis\" (1975).\n",
    "Tensors = loadCombinedArray(cases,'Tensors')\n",
    "print('Shape of basis tensor array: '+str(Tensors.shape))\n",
    "\n",
    "#Load the 47 invariants (N,47) used by Wu et al. \"Physics-informed machine learning approach for augmenting turbulence models: A comprehensive framework\" (2018)\n",
    "Invariants = loadCombinedArray(cases,'I1')\n",
    "print('Shape of invariant features array: '+str(Invariants.shape))\n",
    "\n",
    "#Load the additional scalars (N,5): \n",
    "#    q[:,0] = Ratio of excess rotation to strain rate,\n",
    "#    q[:,1] = Wall-distance based Reynolds number, \n",
    "#    q[:,2] = Ratio of turbulent time scale to mean strain time scale\n",
    "#    q[:,3] = Ratio of total Reynolds stress to 1/2 * normal Reynolds stress (TKE)\n",
    "# f = \"../data/kepsilon/\"\n",
    "Scalars = loadCombinedArray(cases,'q')\n",
    "print('Shape of scalar features array: '+str(Scalars.shape))\n",
    "\n",
    "#Combine the invariants and scalars to form a feature set\n",
    "Features = np.column_stack((Invariants,Scalars))\n",
    "print('Shape of combined features array: '+str(Features.shape))\n",
    "\n",
    "#Optional: remove outliers based on the number of standard deviations away from the mean. \n",
    "#Note: must be careful, as there are naturally large variations in flow features. Even a 5*stdev critera removes many valid near-wall points.\n",
    "def remove_outliers(Features):\n",
    "    stdev = np.std(Features,axis=0)\n",
    "    means = np.mean(Features,axis=0)\n",
    "    ind_drop = np.empty(0)\n",
    "    for i in range(len(Features[0,:])):\n",
    "        ind_drop = np.concatenate((ind_drop,np.where((Features[:,i]>means[i]+5*stdev[i]) | (Features[:,i]<means[i]-5*stdev[i]) )[0]))\n",
    "    return ind_drop.astype(int)\n",
    "\n",
    "outlier_removal_switch = 0\n",
    "if outlier_removal_switch == 1:\n",
    "    outlier_index = remove_outliers(Features)\n",
    "    print('Found '+str(len(outlier_index))+' outliers in the input feature set')\n",
    "    Features = np.delete(Features,outlier_index,axis=0)\n",
    "    Tensors = np.delete(Tensors,outlier_index,axis=0)\n",
    "    Labels = np.delete(Labels,outlier_index,axis=0)\n",
    "\n",
    "#Load the label set from DNS/LES:\n",
    "Labels = loadLabels(cases,'b')\n",
    "#If desired, reshape the 3x3 symmetric anisotropy tensor into a 1x6 vector\n",
    "Labels = np.delete(Labels.reshape((len(Labels),9)),[3,6,7],axis=1)\n",
    "print('Shape of DNS/LES labels array: '+str(Labels.shape))\n",
    "\n",
    "# Split the datasets into training and validation\n",
    "indices = np.arange(Features.shape[0])\n",
    "input_shape = Features.shape[1]\n",
    "\n",
    "x_train, x_val, y_train, y_val, ind_train, ind_val = train_test_split(Features, Labels, indices, test_size=0.2, random_state=10, shuffle=True)\n",
    "\n",
    "basis_train = Tensors[ind_train]\n",
    "basis_val = Tensors[ind_val]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "\n",
    "print(' ')\n",
    "print('Training features:')\n",
    "print(x_train.shape)\n",
    "print('Training tensor basis:')\n",
    "print(basis_train.shape)\n",
    "print('Training labels:')\n",
    "print(y_train.shape)\n",
    "print(' ')\n",
    "print('Validation features:')\n",
    "print(x_val.shape)\n",
    "print('Validation tensor basis:')\n",
    "print(basis_val.shape)\n",
    "print('Validation labels:')\n",
    "print(y_val.shape)\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008725,
     "end_time": "2021-03-21T17:47:31.542308",
     "exception": false,
     "start_time": "2021-03-21T17:47:31.533583",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Creating a tensor basis neural network (TBNN)\n",
    "The following model has 3 hidden layers, with 40 neurons each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T17:47:31.575969Z",
     "iopub.status.busy": "2021-03-21T17:47:31.575310Z",
     "iopub.status.idle": "2021-03-21T17:47:37.589743Z",
     "shell.execute_reply": "2021-03-21T17:47:37.588987Z"
    },
    "papermill": {
     "duration": 6.038645,
     "end_time": "2021-03-21T17:47:37.589943",
     "exception": false,
     "start_time": "2021-03-21T17:47:31.551298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "keras.backend.clear_session()\n",
    "\n",
    "#The model has two inputs, a set of input features with a learned mapping, and the tensor basis layer\n",
    "input_layer = keras.layers.Input(shape=(input_shape),name ='input_layer')\n",
    "input_tensor_basis = keras.layers.Input(shape=(10,3,3),name='Tensor_input_layer')\n",
    "\n",
    "#Hidden layer definition\n",
    "hidden1 = keras.layers.Dense(20,name='Hidden1', kernel_initializer=\"lecun_normal\",kernel_regularizer=tf.keras.regularizers.l2(1E-3), activation = \"selu\")(input_layer)\n",
    "hidden2 = keras.layers.Dense(20,name='Hidden2', kernel_initializer=\"lecun_normal\",kernel_regularizer=tf.keras.regularizers.l2(1E-3), activation = \"selu\")(hidden1)\n",
    "hidden3 = keras.layers.Dense(20,name='Hidden3', kernel_initializer=\"lecun_normal\",kernel_regularizer=tf.keras.regularizers.l2(1E-3), activation = \"selu\")(hidden2)\n",
    "\n",
    "#The layer of gn, which are coefficients for each of the ten Tn\n",
    "gn = keras.layers.Dense(10,name='gn', kernel_initializer=\"lecun_normal\",kernel_regularizer=tf.keras.regularizers.l2(1E-4), activation = \"selu\")(hidden3)\n",
    "\n",
    "#Multiply the gn by Tn, with the output being the anisotropy tensor\n",
    "shaped = keras.layers.Reshape((10,1,1),name='Shape_for_dot_product')(gn)\n",
    "merge = keras.layers.Dot(axes=1, name='Dot_product')([shaped,input_tensor_basis])\n",
    "\n",
    "#Reshape the output anisotropy tensor, and trim out duplicate values (it is a symmetric matrix). The end result is a 6 component vector.\n",
    "shaped_output = keras.layers.Reshape((9,1),name='Shaped_output')(merge)\n",
    "trimmed_output1 = keras.layers.Lambda(lambda x : x[:,0])(shaped_output)\n",
    "trimmed_output2 = keras.layers.Lambda(lambda x : x[:,1])(shaped_output)\n",
    "trimmed_output3 = keras.layers.Lambda(lambda x : x[:,2])(shaped_output)\n",
    "trimmed_output4 = keras.layers.Lambda(lambda x : x[:,4])(shaped_output)\n",
    "trimmed_output5 = keras.layers.Lambda(lambda x : x[:,5])(shaped_output)\n",
    "trimmed_output6 = keras.layers.Lambda(lambda x : x[:,8])(shaped_output)\n",
    "merged_output = tf.keras.layers.Concatenate()([trimmed_output1,trimmed_output2,trimmed_output3,trimmed_output4,trimmed_output5,trimmed_output6])\n",
    "\n",
    "\n",
    "model=keras.Model(inputs=[input_layer, input_tensor_basis], outputs=[merged_output])\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate = 5E-4, clipnorm=1000)\n",
    "model.compile(optimizer,loss='mse',metrics=['mae', 'mse'])\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009164,
     "end_time": "2021-03-21T17:47:37.608437",
     "exception": false,
     "start_time": "2021-03-21T17:47:37.599273",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train the tensor basis neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T17:47:37.635233Z",
     "iopub.status.busy": "2021-03-21T17:47:37.634575Z",
     "iopub.status.idle": "2021-03-21T18:12:29.757159Z",
     "shell.execute_reply": "2021-03-21T18:12:29.756597Z"
    },
    "papermill": {
     "duration": 1492.139511,
     "end_time": "2021-03-21T18:12:29.757302",
     "exception": false,
     "start_time": "2021-03-21T17:47:37.617791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reduce_lr =tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"loss\",\n",
    "    factor=0.3,\n",
    "    patience=10,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    min_delta=0.0001,\n",
    "    cooldown=0,\n",
    "    min_lr=0,\n",
    ")\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=40,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "history = model.fit([x_train,basis_train], y_train, \n",
    "                    batch_size=1000,\n",
    "                    epochs=1000, \n",
    "                    validation_data = ([x_val,basis_val],y_val), \n",
    "                    verbose=1, \n",
    "                    callbacks=[\n",
    "                            early_stop,\n",
    "                            reduce_lr\n",
    "                            ]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T18:12:45.279658Z",
     "iopub.status.busy": "2021-03-21T18:12:45.279011Z",
     "iopub.status.idle": "2021-03-21T18:13:06.181403Z",
     "shell.execute_reply": "2021-03-21T18:13:06.181883Z"
    },
    "papermill": {
     "duration": 28.613608,
     "end_time": "2021-03-21T18:13:06.182046",
     "exception": false,
     "start_time": "2021-03-21T18:12:37.568438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.evaluate([x_train,basis_train],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T18:13:21.865637Z",
     "iopub.status.busy": "2021-03-21T18:13:21.864470Z",
     "iopub.status.idle": "2021-03-21T18:13:27.097030Z",
     "shell.execute_reply": "2021-03-21T18:13:27.096446Z"
    },
    "papermill": {
     "duration": 13.134832,
     "end_time": "2021-03-21T18:13:27.097194",
     "exception": false,
     "start_time": "2021-03-21T18:13:13.962362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.evaluate([x_val,basis_val],y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T18:13:42.757956Z",
     "iopub.status.busy": "2021-03-21T18:13:42.757282Z",
     "iopub.status.idle": "2021-03-21T18:13:43.119355Z",
     "shell.execute_reply": "2021-03-21T18:13:43.118703Z"
    },
    "papermill": {
     "duration": 8.169477,
     "end_time": "2021-03-21T18:13:43.119497",
     "exception": false,
     "start_time": "2021-03-21T18:13:34.950020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rand_ind = np.random.randint(0,len(x_train),5)\n",
    "print('Picked 5 random indices: ' +str(rand_ind))\n",
    "\n",
    "for i in range(len(rand_ind)):\n",
    "    print('Index: '+str(rand_ind[i]))\n",
    "    print('Label anisotropy values: ')\n",
    "    print(y_train[rand_ind[i],:])\n",
    "    print('Model prediction: ')\n",
    "    print(model.predict([x_train[rand_ind[i],:].reshape(1,x_train.shape[1]),basis_train[rand_ind[i],:,:,:].reshape(1,10,3,3)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1598.801018,
   "end_time": "2021-03-21T18:13:52.258681",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-03-21T17:47:13.457663",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
